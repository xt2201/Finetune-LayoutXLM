# Finetune LayoutXLM for Document Layout Analysis# LayoutXLM Fine-tuning for Document Layout Analysis



Fine-tuning LayoutXLM model for Document Layout Analysis using real OCR data from Tesseract.Dá»± Ã¡n fine-tune LayoutXLM cho bÃ i toÃ¡n phÃ¢n tÃ­ch layout tÃ i liá»‡u (Document Layout Analysis).



## ğŸ¯ Project Overview## Cáº¥u trÃºc dá»± Ã¡n



This project fine-tunes the LayoutXLM model for document layout analysis tasks. It uses **real OCR data** extracted via Tesseract to train the model on document structure understanding.```

dlo/

### Key Featuresâ”œâ”€â”€ config.yml              # File cáº¥u hÃ¬nh training

â”œâ”€â”€ train.py               # Script training chÃ­nh

- âœ… **Real OCR Integration**: Uses Tesseract OCR to extract text and bounding boxesâ”œâ”€â”€ dataset.py             # Module xá»­ lÃ½ dataset

- âœ… **LayoutXLM Model**: Fine-tuning microsoft/layoutxlm-base for token classificationâ”œâ”€â”€ utils.py               # CÃ¡c hÃ m tiá»‡n Ã­ch

- âœ… **8-Class Document Layout**: Classifies document regions into 8 layout categoriesâ”œâ”€â”€ requirements.txt       # Dependencies

- âœ… **Mixed Precision Training**: Supports FP16 for faster trainingâ”œâ”€â”€ LayoutXLM.md          # TÃ i liá»‡u vá» LayoutXLM

- âœ… **Wandb Integration**: Full experiment tracking and loggingâ”œâ”€â”€ Data_Description.md   # MÃ´ táº£ dataset

- âœ… **Early Stopping**: Prevents overfitting with patience-based stoppingâ”œâ”€â”€ data/                 # ThÆ° má»¥c dá»¯ liá»‡u

- âœ… **Comprehensive Testing**: Component tests for validation before trainingâ”œâ”€â”€ log/                  # ThÆ° má»¥c logs

â””â”€â”€ checkpoints/          # ThÆ° má»¥c lÆ°u checkpoints

## ğŸ“ Project Structure```



```## CÃ i Ä‘áº·t

.

â”œâ”€â”€ config.yml                  # Training configuration### 1. Táº¡o mÃ´i trÆ°á»ng Python

â”œâ”€â”€ train.py                   # Main training script

â”œâ”€â”€ dataset_ocr.py             # OCR-integrated dataset```bash

â”œâ”€â”€ utils.py                   # Utility functions# Táº¡o virtual environment

â”œâ”€â”€ test_components.py         # Component validation testspython -m venv venv

â”œâ”€â”€ requirements.txt           # Python dependencies

â”œâ”€â”€ verify_data_pipeline.py    # Data pipeline verification script# KÃ­ch hoáº¡t environment

â”œâ”€â”€ doc/                       # Documentationsource venv/bin/activate  # Linux/Mac

â”‚   â”œâ”€â”€ DATA_PIPELINE_STATUS.md# hoáº·c

â”‚   â”œâ”€â”€ VERIFICATION_COMPLETE.mdvenv\Scripts\activate  # Windows

â”‚   â”œâ”€â”€ XAC_NHAN_DU_LIEU_THAT.md```

â”‚   â”œâ”€â”€ TRAINING_GUIDE.md

â”‚   â””â”€â”€ ...### 2. CÃ i Ä‘áº·t dependencies

â”œâ”€â”€ data/                      # Dataset directory

â”‚   â”œâ”€â”€ train.txt             # Training image list```bash

â”‚   â”œâ”€â”€ val.txt               # Validation image listpip install -r requirements.txt

â”‚   â””â”€â”€ test.txt              # Test image list```

â””â”€â”€ checkpoints/               # Model checkpoints (gitignored)

```## Cáº¥u hÃ¬nh



## ğŸš€ Quick StartTáº¥t cáº£ cáº¥u hÃ¬nh Ä‘Æ°á»£c quáº£n lÃ½ trong file `config.yml`:



### 1. Prerequisites- **Wandb**: API key, project name, entity

- **Model**: Pretrained model, sá»‘ labels, mapping labels

- Python 3.8+- **Data**: ÄÆ°á»ng dáº«n datasets, batch size, preprocessing workers

- CUDA-capable GPU (recommended)- **Training**: Learning rate, epochs, optimizer, scheduler

- Tesseract OCR installed- **Checkpoint**: Táº§n suáº¥t lÆ°u checkpoint, best model

- **Logging**: Log directory, log level, logging frequency

Install Tesseract:

```bashXem chi tiáº¿t trong file `config.yml` vá»›i comments Ä‘áº§y Ä‘á»§.

# Ubuntu/Debian

sudo apt-get install tesseract-ocr## Dataset



# macOSDataset bao gá»“m 12,636 áº£nh tÃ i liá»‡u vá»›i annotations bounding boxes:

brew install tesseract

- **Train**: 10,105 samples

# Windows- **Validation**: 1,262 samples  

# Download from: https://github.com/UB-Mannheim/tesseract/wiki- **Test**: 1,269 samples

```

Format labels: YOLO format vá»›i 8 classes (0-7)

### 2. Installation

Chi tiáº¿t xem file `Data_Description.md`.

```bash

# Clone repository## Huáº¥n luyá»‡n

git clone https://github.com/xt2201/Finetune-LayoutXLM.git

cd Finetune-LayoutXLM### Cháº¡y training



# Create virtual environment```bash

python -m venv venvpython train.py

source venv/bin/activate  # Linux/Mac```

# or: venv\Scripts\activate  # Windows

Script sáº½:

# Install dependencies- âœ“ Load config tá»« `config.yml`

pip install -r requirements.txt- âœ“ Khá»Ÿi táº¡o wandb logging

```- âœ“ Load LayoutXLM pretrained model

- âœ“ Load vÃ  preprocess dataset

### 3. Data Preparation- âœ“ Train model vá»›i progress bars Ä‘áº¹p (Rich)

- âœ“ LÆ°u checkpoint má»—i 5 epochs

Prepare your dataset with the following structure:- âœ“ LÆ°u best model dá»±a trÃªn validation loss

- âœ“ Log metrics vÃ o wandb vÃ  file logs

```

data/### Theo dÃµi training

â”œâ”€â”€ train.txt              # List of training image paths

â”œâ”€â”€ val.txt                # List of validation image paths**1. Console Output:**

â”œâ”€â”€ test.txt               # List of test image paths- Progress bars vá»›i Rich library

â”œâ”€â”€ train/- Báº£ng thÃ´ng tin model, dataset

â”‚   â”œâ”€â”€ images/           # Training images- Metrics má»—i epoch

â”‚   â””â”€â”€ labels/           # YOLO format labels

â”œâ”€â”€ val/**2. Wandb Dashboard:**

â”‚   â”œâ”€â”€ images/- Truy cáº­p: https://wandb.ai/thanhnx/doclayout

â”‚   â””â”€â”€ labels/- Xem real-time metrics, charts

â””â”€â”€ test/- So sÃ¡nh cÃ¡c runs

    â”œâ”€â”€ images/

    â””â”€â”€ labels/**3. Log Files:**

```- Logs Ä‘Æ°á»£c lÆ°u trong `log/train_YYYYMMDD_HHMMSS.log`



Label format (YOLO): `class_id x_center y_center width height`## Checkpoints



### 4. ConfigurationCheckpoints Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c `checkpoints/`:



Edit `config.yml` to customize training:- `checkpoint_epoch_5.pt`: Checkpoint epoch 5

- `checkpoint_epoch_10.pt`: Checkpoint epoch 10

```yaml- `checkpoint_epoch_15.pt`: Checkpoint epoch 15

data:- ...

  use_ocr: true              # Use real OCR (Tesseract)- `best_model.pt`: Best model theo validation loss

  batch_size: 8

  max_length: 512## ÄÃ¡nh giÃ¡



training:Sau khi training, evaluate model trÃªn test set:

  num_epochs: 30

  learning_rate: 0.00005```bash

  gradient_accumulation_steps: 4python evaluate.py --checkpoint checkpoints/best_model.pt

  early_stopping_patience: 5```



wandb:## TÃ­nh nÄƒng ná»•i báº­t

  project: your-project-name

  api_key: your-api-key1. **Clean & Modular Code**: Code tá»• chá»©c rÃµ rÃ ng, dá»… maintain

```2. **Rich Logging**: Progress bars, tables Ä‘áº¹p vá»›i Rich library

3. **Wandb Integration**: Track experiments chuyÃªn nghiá»‡p

### 5. Verify Setup4. **Flexible Config**: Táº¥t cáº£ config trong YAML, dá»… thay Ä‘á»•i

5. **Checkpoint Management**: Auto save checkpoints vÃ  best model

Run component tests to ensure everything is configured correctly:6. **Comprehensive Logging**: Logs Ä‘áº§y Ä‘á»§ vÃ o files



```bash## Tham kháº£o

python test_components.py

```- [LayoutXLM Documentation](LayoutXLM.md)

- [Dataset Description](Data_Description.md)

Expected output:- [Hugging Face LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)

```

âœ“ PASS   - Imports## License

âœ“ PASS   - Config

âœ“ PASS   - DatasetMIT License

âœ“ PASS   - Model
âœ“ PASS   - Forward Pass
âœ“ PASS   - Utilities

Total: 6/6 tests passed
```

### 6. Verify Data Pipeline

Confirm that real OCR data is being used:

```bash
python verify_data_pipeline.py
```

### 7. Start Training

```bash
python train.py
```

## ğŸ“Š Model Architecture

- **Base Model**: microsoft/layoutxlm-base
- **Task**: Token Classification
- **Input**: Text tokens + Bounding boxes + Document images
- **Output**: 8-class layout predictions per token
- **Parameters**: ~368M total, ~368M trainable

### Label Classes

```
0: O (Other/Background)
1-7: Document layout regions (configurable in config.yml)
```

## ğŸ”§ Training Configuration

### Key Hyperparameters

```yaml
# Optimizer
- AdamW with weight decay 0.01
- Learning rate: 5e-5
- Warmup ratio: 6%

# Training
- Batch size: 8 (effective 32 with gradient accumulation)
- Max epochs: 30
- Gradient clipping: 1.0
- Mixed precision: FP16 (optional)

# Regularization
- Early stopping patience: 5 epochs
- Weight decay: 0.01
```

## ğŸ“ˆ Monitoring

Training metrics are logged to Weights & Biases:

- Training loss
- Validation loss and accuracy
- Learning rate schedule
- GPU utilization
- Sample predictions

## ğŸ§ª Data Pipeline

### Real OCR Processing

1. **Image Loading**: Load document images
2. **OCR Extraction**: Tesseract extracts text and word-level bounding boxes
3. **Coordinate Normalization**: Convert to [0, 1000] scale for LayoutXLM
4. **Label Alignment**: Align layout labels to OCR words using IoU
5. **Tokenization**: LayoutXLMProcessor tokenizes with spatial information
6. **Batching**: Custom collate function for proper batching

### Fallback Behavior

When OCR is disabled (`use_ocr: false`):
- Used only for quick component testing
- Returns dummy tokens to verify pipeline without OCR dependency
- **NOT used in actual training** (training uses `use_ocr: true`)

## ğŸ“ Output

### Checkpoints

Saved in `checkpoints/` directory:
- `best_model.pt`: Best model based on validation loss
- `checkpoint_epoch_N.pt`: Periodic checkpoints every N epochs

### Logs

- **Console**: Rich formatted progress bars and metrics
- **File**: Detailed logs in `log/` directory
- **Wandb**: Interactive dashboard with all metrics

## ğŸ” Verification

The project includes comprehensive verification:

1. **Component Tests** (`test_components.py`)
   - Validates all components load correctly
   - Tests model forward pass
   - Checks dataset pipeline

2. **Data Pipeline Verification** (`verify_data_pipeline.py`)
   - Confirms real OCR is being used
   - Checks data quality
   - Validates token diversity

3. **Documentation**
   - `DATA_PIPELINE_STATUS.md`: Technical details
   - `VERIFICATION_COMPLETE.md`: Full verification report
   - `XAC_NHAN_DU_LIEU_THAT.md`: Vietnamese verification

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Tesseract not found**
   ```bash
   # Install Tesseract OCR
   sudo apt-get install tesseract-ocr
   ```

2. **CUDA out of memory**
   - Reduce `batch_size` in config.yml
   - Increase `gradient_accumulation_steps`
   - Reduce `max_length`

3. **Slow training**
   - Enable FP16: Set `fp16: true` in config.yml
   - Increase `num_workers` for data loading

## ğŸ“š Documentation

- `doc/TRAINING_GUIDE.md`: Detailed training guide
- `doc/DATA_PIPELINE_STATUS.md`: Data pipeline documentation
- `doc/VERIFICATION_COMPLETE.md`: Verification report
- `doc/PROJECT_SUMMARY.md`: Project overview

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- [LayoutXLM](https://github.com/microsoft/unilm/tree/master/layoutxlm) by Microsoft
- [Detectron2](https://github.com/facebookresearch/detectron2) by Facebook Research
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)

## ğŸ“§ Contact

- GitHub: [@xt2201](https://github.com/xt2201)
- Email: xuanthanh2201.work@gmail.com

---

**Status**: âœ… Production Ready - All components verified with real OCR data
